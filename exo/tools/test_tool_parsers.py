import pytest
import json
import re
from typing import List, Optional, Dict, Any, Tuple
from exo.tools.tool_parsers import (
  ToolParser,
  WrappedJsonToolParser,
  LlamaPythonTag,
  WattToolParser,
  generate_tool_call_json_schema,
  get_parser_class
)
from exo.tools import ToolDefinition, ToolChoice, SpecificToolChoice, AssistantToolCall
from exo.inference.grammars import lark_grammar
from llguidance import LLInterpreter
from llguidance.hf import from_tokenizer as llg_from_tokenizer
from transformers import AutoTokenizer

# Use pytest.importorskip to handle cases where transformers is not installed
transformers = pytest.importorskip("transformers")


@pytest.fixture(scope="session")
def openai_tokenizer():
  """Load the OpenAI tokenizer for tool_call format"""
  try:
    return AutoTokenizer.from_pretrained("openai/gpt-4o")
  except Exception:
    # Fallback to a more commonly available tokenizer if gpt-4o isn't available
    return AutoTokenizer.from_pretrained("gpt2")


@pytest.fixture(scope="session")
def llama_tokenizer():
  return AutoTokenizer.from_pretrained("mlx-community/Llama-3.2-1B-Instruct-4bit")


@pytest.fixture(scope="session")
def watt_tokenizer():
  """Load the Watt tokenizer for watt format"""
  try:
    return AutoTokenizer.from_pretrained("watt-ai/watt-tool-8B")
  except Exception:
    # Fallback to a more commonly available tokenizer
    return AutoTokenizer.from_pretrained("gpt2")


@pytest.fixture
def sample_tools() -> List[ToolDefinition]:
  return [
    ToolDefinition(
      name="get_weather",
      description="Get the current weather in a location",
      parameters={
        "type": "object",
        "properties": {
          "location": {
            "type": "string",
            "description": "The city and state, e.g. San Francisco, CA"
          },
          "unit": {
            "type": "string",
            "enum": ["celsius", "fahrenheit"],
            "description": "The unit of temperature"
          }
        },
        "required": ["location"]
      }
    ),
    ToolDefinition(
      name="search_database",
      description="Search for information in the database",
      parameters={
        "type": "object",
        "properties": {
          "query": {
            "type": "string",
            "description": "The search query"
          },
          "limit": {
            "type": "integer",
            "description": "Maximum number of results to return"
          }
        },
        "required": ["query"]
      }
    )
  ]


def validate_grammar_parsing(parser: ToolParser, sample_text: str) -> Tuple[bool, Optional[str]]:
  """
  Helper function to test if a sample text can be parsed by the grammar generated by a parser.

  Args:
      parser: The tool parser to test
      sample_text: The sample text to parse

  Returns:
      Tuple of (success, error_message)
  """
  try:
    # Get the grammar from the parser
    grammar_definition = lark_grammar(parser.tool_grammar())

    # Initialize the LLInterpreter
    interpreter = LLInterpreter(
      llg_from_tokenizer(parser.tokenizer, n_vocab=parser.tokenizer.vocab_size),
      grammar_definition,
      enable_ff_tokens=False,
      enable_backtrack=False,
      log_level=2  # Suppress logs
    )

    # Start the interpreter without a prompt
    interpreter.start_without_prompt()

    # Tokenize the sample text
    tokens = parser.tokenizer.encode(sample_text, add_special_tokens=False)

    # Try to commit each token
    for i, token in enumerate(tokens):
      mask, _ = interpreter.compute_mask()

      valid = interpreter.commit_token(token)
      if not valid:
        decoded = parser.tokenizer.decode([token])
        context = parser.tokenizer.decode(tokens[max(0, i - 5):i]) + " [" + decoded + "] " + parser.tokenizer.decode(
          tokens[i + 1:min(len(tokens), i + 5)])
        return False, f"Failed to parse token {token} ('{decoded}') at position {i}. Context: '{context}'"

    # Check if the interpreter has a pending stop
    if not interpreter.has_pending_stop():
      return False, "Parsing completed but no pending stop was detected"

    return True, None
  except Exception as e:
    return False, f"Error testing grammar parsing: {e}"


class TestToolParserBase:
  def test_active_tools_none(self, openai_tokenizer, sample_tools):
    parser = WrappedJsonToolParser(openai_tokenizer, sample_tools, "none")
    assert parser.active_tools() == []

  def test_active_tools_auto(self, openai_tokenizer, sample_tools):
    parser = WrappedJsonToolParser(openai_tokenizer, sample_tools, "auto")
    assert parser.active_tools() == sample_tools

  def test_active_tools_required(self, openai_tokenizer, sample_tools):
    parser = WrappedJsonToolParser(openai_tokenizer, sample_tools, "required")
    assert parser.active_tools() == sample_tools

  def test_active_tools_specific(self, openai_tokenizer, sample_tools):
    specific_choice = SpecificToolChoice(function=SpecificToolChoice.SpecificToolChoiceInner(name="get_weather"))
    parser = WrappedJsonToolParser(openai_tokenizer, sample_tools, specific_choice)
    assert len(parser.active_tools()) == 1
    assert parser.active_tools()[0].name == "get_weather"

  def test_is_immediate_auto(self, openai_tokenizer, sample_tools):
    parser = WrappedJsonToolParser(openai_tokenizer, sample_tools, "auto")
    assert not parser.is_immediate()

  def test_is_immediate_required(self, openai_tokenizer, sample_tools):
    parser = WrappedJsonToolParser(openai_tokenizer, sample_tools, "required")
    assert parser.is_immediate()

  def test_is_immediate_specific(self, openai_tokenizer, sample_tools):
    specific_choice = SpecificToolChoice(function=SpecificToolChoice.SpecificToolChoiceInner(name="get_weather"))
    parser = WrappedJsonToolParser(openai_tokenizer, sample_tools, specific_choice)
    assert parser.is_immediate()


class TestWrappedJsonToolParser:
  def test_start_token(self, openai_tokenizer, sample_tools):
    parser = WrappedJsonToolParser(openai_tokenizer, sample_tools, "auto")
    # Just verify it returns a token ID
    assert isinstance(parser.start_token(), int)

  def test_tool_grammar(self, openai_tokenizer, sample_tools):
    parser = WrappedJsonToolParser(openai_tokenizer, sample_tools, "auto")
    grammar = parser.tool_grammar()
    assert "%llguidance" in grammar
    assert "start: <tool_call> json_body </tool_call>" in grammar
    assert "json_body: %json" in grammar
    # Check that tool names are in the grammar
    assert "get_weather" in grammar
    assert "search_database" in grammar

  def test_parse_tool_calls(self, openai_tokenizer, sample_tools):
    parser = WrappedJsonToolParser(openai_tokenizer, sample_tools, "auto")
    content = """Some text before
<tool_call>
{"name": "get_weather", "arguments": "{\\"location\\": \\"San Francisco, CA\\", \\"unit\\": \\"celsius\\"}"}
</tool_call>
Some text after"""

    remaining, tool_calls = parser.parse_tool_calls(content)

    assert len(tool_calls) == 1
    assert tool_calls[0].name == "get_weather"
    assert json.loads(tool_calls[0].arguments)["location"] == "San Francisco, CA"
    assert "Some text after" in remaining

  def test_grammar_parsing_valid(self, openai_tokenizer, sample_tools):
    parser = WrappedJsonToolParser(openai_tokenizer, sample_tools, "auto")
    valid_sample = '<tool_call>\n{"name": "get_weather", "arguments": "{\\"location\\": \\"San Francisco, CA\\"}"}\n</tool_call>'

    success, error = validate_grammar_parsing(parser, valid_sample)
    if not success:
      pytest.fail(f"Grammar validation failed: {error}")

    assert success

  def test_grammar_parsing_invalid_tool(self, openai_tokenizer, sample_tools):
    parser = WrappedJsonToolParser(openai_tokenizer, sample_tools, "auto")
    # Invalid tool name
    invalid_sample = '<tool_call>\n{"name": "invalid_tool", "arguments": "{\\"location\\": \\"San Francisco, CA\\"}"}\n</tool_call>'

    success, _ = validate_grammar_parsing(parser, invalid_sample)
    assert not success, "Grammar should reject invalid tool names"


class TestLlamaPythonTag:
  def test_start_token(self, llama_tokenizer, sample_tools):
    parser = LlamaPythonTag(llama_tokenizer, sample_tools, "auto")
    # Just verify it returns a token ID
    assert isinstance(parser.start_token(), int)

  def test_tool_grammar(self, llama_tokenizer, sample_tools):
    parser = LlamaPythonTag(llama_tokenizer, sample_tools, "auto")
    grammar = parser.tool_grammar()
    assert "%llguidance" in grammar
    assert "fun_call: <|python_tag|> json_body <|eom_id|>" in grammar
    assert "json_body: %json" in grammar
    # Check that tool names are in the grammar
    assert "get_weather" in grammar
    assert "search_database" in grammar
    # Check that it uses "parameters" instead of "arguments"
    assert "parameters" in grammar

  def test_parse_tool_calls(self, llama_tokenizer, sample_tools):
    parser = LlamaPythonTag(llama_tokenizer, sample_tools, "auto")
    content = """Some text before
<|python_tag|>{"name": "get_weather", "parameters": {"location": "San Francisco, CA", "unit": "celsius"}}<|eom_id|>
Some text after"""

    remaining, tool_calls = parser.parse_tool_calls(content)

    assert len(tool_calls) == 1
    assert tool_calls[0].name == "get_weather"
    assert json.loads(tool_calls[0].arguments)["location"] == "San Francisco, CA"
    assert "Some text after" in remaining

  def test_grammar_parsing_valid(self, llama_tokenizer, sample_tools):
    parser = LlamaPythonTag(llama_tokenizer, sample_tools, "auto")
    valid_sample = '<|python_tag|>{"name": "get_weather", "parameters": {"location": "San Francisco, CA"}}<|eom_id|>'

    success, error = validate_grammar_parsing(parser, valid_sample)
    if not success:
      pytest.fail(f"Grammar validation failed: {error}")

    assert success

  def test_grammar_parsing_invalid_tool(self, llama_tokenizer, sample_tools):
    parser = LlamaPythonTag(llama_tokenizer, sample_tools, "auto")
    # Invalid tool name
    invalid_sample = '<|python_tag|>{"name": "invalid_tool", "parameters": {"location": "San Francisco, CA"}}<|eom_id|>'

    success, _ = validate_grammar_parsing(parser, invalid_sample)
    assert not success, "Grammar should reject invalid tool names"


class TestWattToolParser:
  def test_start_token(self, watt_tokenizer, sample_tools):
    parser = WattToolParser(watt_tokenizer, sample_tools, "auto")
    # Just verify it returns a token ID
    assert isinstance(parser.start_token(), int)

  def test_tool_grammar(self, watt_tokenizer, sample_tools):
    parser = WattToolParser(watt_tokenizer, sample_tools, "auto")
    grammar = parser.tool_grammar()
    assert "%llguidance" in grammar
    assert 'start: "[" function_call "]"' in grammar
    assert "function_name:" in grammar
    # Check that tool names are in the grammar
    assert "get_weather" in grammar
    assert "search_database" in grammar

  def test_parse_tool_calls(self, watt_tokenizer, sample_tools):
    parser = WattToolParser(watt_tokenizer, sample_tools, "auto")
    content = """Some text before
[get_weather(location="San Francisco, CA", unit="celsius")]
Some text after"""

    remaining, tool_calls = parser.parse_tool_calls(content)

    assert len(tool_calls) == 1
    assert tool_calls[0].name == "get_weather"
    assert json.loads(tool_calls[0].arguments)["location"] == "San Francisco, CA"
    assert "Some text after" in remaining

  def test_parse_complex_parameters(self, watt_tokenizer, sample_tools):
    parser = WattToolParser(watt_tokenizer, sample_tools, "auto")
    content = """[search_database(query="complex query", limit=5, filters={"date": "2023-01-01", "tags": ["climate", "weather"]})]"""

    remaining, tool_calls = parser.parse_tool_calls(content)

    assert len(tool_calls) == 1
    args = json.loads(tool_calls[0].arguments)
    assert args["query"] == "complex query"
    assert args["limit"] == 5
    assert args["filters"]["date"] == "2023-01-01"
    assert "climate" in args["filters"]["tags"]

  def test_grammar_parsing_valid(self, watt_tokenizer, sample_tools):
    parser = WattToolParser(watt_tokenizer, sample_tools, "auto")
    valid_sample = '[get_weather(location="San Francisco, CA")]'

    success, error = validate_grammar_parsing(parser, valid_sample)
    if not success:
      pytest.fail(f"Grammar validation failed: {error}")

    assert success

  def test_grammar_parsing_complex(self, watt_tokenizer, sample_tools):
    parser = WattToolParser(watt_tokenizer, sample_tools, "auto")
    complex_sample = '[search_database(query="climate data", limit=5)]'

    success, error = validate_grammar_parsing(parser, complex_sample)
    if not success:
      pytest.fail(f"Grammar validation failed: {error}")

    assert success

  def test_grammar_parsing_invalid_tool(self, watt_tokenizer, sample_tools):
    parser = WattToolParser(watt_tokenizer, sample_tools, "auto")
    # Invalid tool name
    invalid_sample = '[invalid_tool(location="San Francisco, CA")]'

    success, _ = validate_grammar_parsing(parser, invalid_sample)
    assert not success, "Grammar should reject invalid tool names"


class TestGrammarWithSpecificToolChoice:
  def test_specific_tool_grammar(self, openai_tokenizer, sample_tools):
    specific_choice = SpecificToolChoice(function=SpecificToolChoice.SpecificToolChoiceInner(name="get_weather"))
    parser = WrappedJsonToolParser(openai_tokenizer, sample_tools, specific_choice)

    # This should pass because get_weather is the specific tool
    valid_sample = '<tool_call>\n{"name": "get_weather", "arguments": "{\\"location\\": \\"San Francisco, CA\\"}"}\n</tool_call>'
    success, error = validate_grammar_parsing(parser, valid_sample)
    if not success:
      pytest.fail(f"Grammar validation failed: {error}")

    assert success

    # This should fail because search_database is not the specific tool
    invalid_sample = '<tool_call>\n{"name": "search_database", "arguments": "{\\"query\\": \\"climate data\\"}"}\n</tool_call>'
    success, _ = validate_grammar_parsing(parser, invalid_sample)
    assert not success, "Grammar should reject tools not in the specific choice"

  def test_specific_tool_grammar_llama(self, llama_tokenizer, sample_tools):
    specific_choice = SpecificToolChoice(function=SpecificToolChoice.SpecificToolChoiceInner(name="get_weather"))
    parser = LlamaPythonTag(llama_tokenizer, sample_tools, specific_choice)

    # This should pass because get_weather is the specific tool
    valid_sample = '<|python_tag|>{"name": "get_weather", "parameters": {"location": "San Francisco, CA"}}<|eom_id|>'
    success, error = validate_grammar_parsing(parser, valid_sample)
    if not success:
      pytest.fail(f"Grammar validation failed: {error}")

    assert success

    # This should fail because search_database is not the specific tool
    invalid_sample = '<|python_tag|>{"name": "search_database", "parameters": {"query": "climate data"}}<|eom_id|>'
    success, _ = validate_grammar_parsing(parser, invalid_sample)
    assert not success, "Grammar should reject tools not in the specific choice"

  def test_specific_tool_grammar_watt(self, watt_tokenizer, sample_tools):
    specific_choice = SpecificToolChoice(function=SpecificToolChoice.SpecificToolChoiceInner(name="get_weather"))
    parser = WattToolParser(watt_tokenizer, sample_tools, specific_choice)

    # This should pass because get_weather is the specific tool
    valid_sample = '[get_weather(location="San Francisco, CA")]'
    success, error = validate_grammar_parsing(parser, valid_sample)
    if not success:
      pytest.fail(f"Grammar validation failed: {error}")

    assert success

    # This should fail because search_database is not the specific tool
    invalid_sample = '[search_database(query="climate data")]'
    success, _ = validate_grammar_parsing(parser, invalid_sample)
    assert not success, "Grammar should reject tools not in the specific choice"


class TestBufferedOutputIntegration:
  def test_buffered_output_with_tool_parser(self, openai_tokenizer, llama_tokenizer, watt_tokenizer, sample_tools):
    from exo.orchestration.buffered_output import BufferedOutput

    # Test with WrappedJsonToolParser
    parser = WrappedJsonToolParser(openai_tokenizer, sample_tools, "auto")
    buffered_output = BufferedOutput(
      max_tokens=100,
      eos_token_id=openai_tokenizer.eos_token_id,
      stop_sequences=[],
      tokenizer=openai_tokenizer,
      tool_parser=parser
    )

    # Verify that the guidance interpreter was initialized
    assert buffered_output.guidance_interpreter is not None

    # Test with LlamaPythonTag
    parser = LlamaPythonTag(llama_tokenizer, sample_tools, "auto")
    buffered_output = BufferedOutput(
      max_tokens=100,
      eos_token_id=llama_tokenizer.eos_token_id,
      stop_sequences=[],
      tokenizer=llama_tokenizer,
      tool_parser=parser
    )

    # Verify that the guidance interpreter was initialized
    assert buffered_output.guidance_interpreter is not None

    # Test with WattToolParser
    parser = WattToolParser(watt_tokenizer, sample_tools, "auto")
    buffered_output = BufferedOutput(
      max_tokens=100,
      eos_token_id=watt_tokenizer.eos_token_id,
      stop_sequences=[],
      tokenizer=watt_tokenizer,
      tool_parser=parser
    )

    # Verify that the guidance interpreter was initialized
    assert buffered_output.guidance_interpreter is not None


class TestGenerateToolCallJsonSchema:
  def test_single_tool_schema(self, sample_tools):
    schema = generate_tool_call_json_schema([sample_tools[0]])
    assert schema["type"] == "object"
    assert "name" in schema["properties"]
    assert "arguments" in schema["properties"]
    assert schema["properties"]["name"]["enum"] == ["get_weather"]

  def test_multiple_tools_schema(self, sample_tools):
    schema = generate_tool_call_json_schema(sample_tools)
    assert "oneOf" in schema
    assert len(schema["oneOf"]) == 2
    tool_names = [tool_schema["properties"]["name"]["enum"][0] for tool_schema in schema["oneOf"]]
    assert "get_weather" in tool_names
    assert "search_database" in tool_names

  def test_custom_parameter_key(self, sample_tools):
    schema = generate_tool_call_json_schema(sample_tools, parameter_key="params")
    assert "oneOf" in schema
    assert "params" in schema["oneOf"][0]["properties"]
    assert "arguments" not in schema["oneOf"][0]["properties"]

  def test_empty_tools_error(self):
    with pytest.raises(ValueError):
      generate_tool_call_json_schema([])


class TestGetParserClass:
  def test_get_tool_call_parser(self):
    parser_class = get_parser_class("tool_call")
    assert parser_class == WrappedJsonToolParser

  def test_get_llama_json_parser(self):
    parser_class = get_parser_class("llama_json")
    assert parser_class == LlamaPythonTag

  def test_get_watt_parser(self):
    parser_class = get_parser_class("watt")
    assert parser_class == WattToolParser

  def test_unknown_parser_error(self):
    with pytest.raises(ValueError):
      get_parser_class("unknown_parser")


class TestGrammarEdgeCases:
  """Test edge cases for grammar parsing."""

  def test_empty_parameter_list(self, watt_tokenizer, sample_tools):
    """Test that empty parameter lists are handled correctly."""
    parser = WattToolParser(watt_tokenizer, sample_tools, "auto")
    empty_params_sample = '[get_weather()]'

    success, error = validate_grammar_parsing(parser, empty_params_sample)
    if not success:
      pytest.fail(f"Grammar validation failed: {error}")

    assert success

  def test_whitespace_handling(self, watt_tokenizer, sample_tools):
    """Test that whitespace is handled correctly."""
    parser = WattToolParser(watt_tokenizer, sample_tools, "auto")
    whitespace_sample = '[get_weather(  location  =  "San Francisco, CA")  ]'

    success, error = validate_grammar_parsing(parser, whitespace_sample)
    if not success:
      pytest.fail(f"Grammar validation failed: {error}")

    assert success

  def test_nested_json_objects(self, openai_tokenizer, sample_tools):
    """Test that nested JSON objects are handled correctly."""
    parser = WrappedJsonToolParser(openai_tokenizer, sample_tools, "auto")
    nested_json_sample = '<tool_call>\n{"name": "search_database", "arguments": "{\\"query\\": \\"climate data\\", \\"filters\\": {\\"date\\": \\"2023-01-01\\", \\"tags\\": [\\"climate\\", \\"weather\\"]}}"}\n</tool_call>'

    success, error = validate_grammar_parsing(parser, nested_json_sample)
    if not success:
      pytest.fail(f"Grammar validation failed: {error}")

    assert success
